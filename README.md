# Optimizers Fullscratch

## Description
This is a repository for implementing optimizers from scratch.

## Implemented Optimizers
- [x] Gradient Descent
- [x] Momentum
- [x] RMSprop
- [x] Adam

## Results

### results of each optimizer
| Optimizer | total iteration | learning rate |
|:---------:|:---------------:|:------------:|
| Gradient Descent | 12768 | 0.002 |
| Momentum | 722 | 0.03 |
| RMSprop | 9842 | 0.0001 |
| Adam | 830 | 0.02 |

### learning curve of each optimizer
![learning curve](/results/loss_curve.png)
